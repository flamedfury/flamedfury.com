---
title: Blocking Bots With 11ty And Apache
description:
tags:
  - eleventy
  - development
  - guides
date: 2024-06-16
timestamp: 2024-06-16T10:09:00.577Z
---

What's going on Internet? There's a lot of Internet discourse recently about AI scraping the web to use all of our words to power their language models to then sell us AI powered products that we never asked for.

Flamed Fury is hosted on a [shared hosting plan](https://flamedfury.com/posts/deploying-an-11ty-project-to-shared-hosting/) which is Apache based and has access to the .htaccess file to suppplement the robots.txt.

While reading Ethan Marcotte's post, [Blockin Bots](https://ethanmarcotte.com/wrote/blockin-bots/) showing how to do this with .htaccess and Jekyll I figured it should be relatively easy to do with 11ty too.

I had a very basic robots.txt already that did the bare minimum but it needed more like Evan's example. [Cory](https://coryd.dev/) maintains an open source list of [known hostile robots](https://github.com/ai-robots-txt/ai.robots.txt/blob/main/robots.txt) and I figured I should be able to fetch the content of his list to be able to generate my robots.txt and .htaccess pages.

On Saturday morning I was banging my head on the wall trying to figure out how to use eleventy-fetch to fetch the list from Cory's repo and make it available as an 11ty data file when I saw Robb share his post, [Blocking Bots With Nginix](https://rknight.me/blog/blocking-bots-with-nginx/) which included the [data file](https://github.com/rknightuk/rknight.me/blob/master/src/_data/site/robots.js) that did exactly what I was trying to do so I jacked it.

The key changes I made was remove the filter for the Apple bot and swap out the Nginix branding

```js
    const data = {
        txt: txt,
        htaccess: bots.join('|'),
    };
```

Now that I had the data available I just needed to update my robots page and create a new page for the .htaccess file.

Here's `/pages/robots.njk`

{% raw %}
```jinja2
---
permalink: /robots.txt
eleventyExcludeFromCollections: true
excludeFromSitemap: true
---

User-agent: ia_archiver
User-agent: MojeekBot
User-agent: search.marginalia.nu
Disallow:

{{ robots.txt }}

Sitemap: {{ meta.url }}/sitemap.xml

```
{% endraw %}

I've included some specific allows for friendly scrapers after reading Starbreaker's [robots.txt: the Nuclear Option](https://starbreaker.org/blog/tech/robots-txt-nuclear-option/index.html) post.

To generate `.htaccess` I created another Nunjucks page `/pages/htaccess.njk`

{% raw %}
```jinja2
---
permalink: /.htaccess
eleventyExcludeFromCollections: true
excludeFromSitemap: true
---
##
# Automatically generated by {{ eleventy.generator }}

<IfModule mod_rewrite.c>
  RewriteEngine on
  RewriteBase /

  # Block ‚ÄúAI‚Äù bots
  RewriteCond %{HTTP_USER_AGENT} {{ robots.htaccess }} [NC]
  RewriteRule ^ ‚Äì [F]
</IfModule>
```
{% endraw %}

What this does is creates a new page `.htaccess` at the site's root. You won't be able to view it based on the it being a hidden file at the server level. It is excluded from collections and the sitemap. It checks that `mod_rewrite` is enabled and turns the `RewriteEngine` on and sets the `RewriteBase` to the base url `/`.

Then the hostile bots list is generated from `{{ robots.htaccess }},`, `[NC]` indicates the case matching doesn't matter and then the `RewriteRule ^ ‚Äì [F]` matches any URI and forces the server to return a 403 Forbidden code and access is blocked.

The final piece was to create a [403 page](/403.shtml) for a laugh, check it out. On Apache you have to make sure to use the `.shtml` extension as that's the default for the error pages.

{% raw %}
```jinja2
---
title: Forbidden
layout: page
permalink: /403.shtml
eleventyExcludeFromCollections: true
excludeFromSitemap: true
noindex: true
---

## Bite my shiny metal ass!

![Bender from Futurama telling AI scraper bots to bite his shiny metal ass](/assets/images/template/bender-bite-my-shiny-metal.png)

Sorry bots, you're not welcome here ü§£
```
{% endraw %}

There you go. Thanks to [Ethan](https://ethanmarcotte.com/), [Robb](https://rknight.me/), [Starbreaker](https://starbreaker.org), and [Cory](https://coryd.dev/) for the starting blocks.